{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<a href=\"https://colab.research.google.com/github/marcoteran/deeplearningmodule/blob/main/06_dataaugmentationandtransferlearning/06_dataaugmentationandtransferlearning.ipynb\" target=\"_blank\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\" title=\"Abrir y ejecutar en Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Upj99k3gOKrQ"
   },
   "source": [
    "## Ejemplos de código\n",
    "# Deep Learning:  *Data Augmentation* y *Transfer Learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Name:* Marco Teran\n",
    "- *E-mail:* marco.teran@usa.edu.co\n",
    "- [Website](http://marcoteran.github.io/), [Github](https://github.com/marcoteran), [LinkedIn](https://www.linkedin.com/in/marcoteran/).\n",
    "\n",
    "\n",
    "[**Slide del tema** ](https://github.com/marcoteran/deeplearningmodule/raw/main/05_deeplearning_generativeadversialnetworks.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar librerías importantes\n",
    "\n",
    "Empezamos con las importaciones estándar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QsdNz2pwOMIK",
    "outputId": "5511537e-d305-49b3-b5d3-c6577a6700b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0-rc1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Data Augmentation: una de las técnicas más populares para mitigar el sobreajuste en los modelos de redes neuronales. En nuestro caso de estudio, esta técnica mejorará el modelo del capítulo anterior, que presentaba una precisión de 73.9 %, alcanzando una precisión de 80.1 %. Transfer Learning: permitirá mejorar aún más el modelo anterior.\n",
    "Primero presentaremos la técnica Feature Extraction usando una red preentrenada que nos llevará a una precisión de 90.4 % y, después, la técnica Fine-Tuning de una red preentrenada que nos llevará a una precisión de 93.1 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Cuantos menos datos tengamos, menos datos para entrenar y menos posibilidades tendremos de obtener predicciones precisas para datos que nuestro modelo aún no ha visto. Data Augmentation adopta el enfoque de generar más datos de entrenamiento a partir de nuestros datos disponibles. En el caso de imágenes, esto lo consigue aplicando una serie de transformaciones aleatorias a la imagen que producen nuevas imágenes de aspecto creíble. Esta idea simple, pero potente, ayuda a exponer el modelo a más aspectos de los datos y a generalizar mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformaciones de imágenes\n",
    "\n",
    "Esta técnica es especialmente poderosa para datos de tipo imagen pero, a la vez, es muy simple, ya que aplica transformaciones sencillas (rotar, voltear...) a las imágenes para obtener nuevas imágenes plausibles, que podrían estar en el conjunto de datos original.\n",
    "No obstante, debe tenerse cuidado con la elección de las técnicas específicas de aumento de datos utilizadas. Hay que tener en cuenta el contexto del conjunto de datos de entrenamiento y el conocimiento del dominio del problema, para no generar imágenes que nunca podrían encontrarse en realidad ya que, de esta manera, estaríamos empeorando el entrenamiento.\n",
    "\n",
    "En Keras, esto se puede hacer de manera muy fácil mediante la configuración de una serie de transformaciones que se realizarán en las imágenes leídas por la instancia de ``ImageDataGenerator``.\n",
    "\n",
    "Para obtener información detallada sobre las transformaciones disponibles puede consultar la API preprocessing de Keras.\n",
    "\n",
    "Es importante resaltar que se realiza la transformación de manera onlinedurante el procesamiento, lo cual permite hacer el proceso automático mientras se realiza el entrenamiento sin necesidad de modificar los datos almacenados en disco. Así, el modelo ve una imagen generada aleatoriamente una sola vez. Evidentemente, estas transformaciones se podrían realizar previamente e incluirse en el conjunto de datos de entrenamiento. De esta manera, el preprocesado resultaría más rápido, puesto que no se realizarían las transformaciones en tiempo de ejecución; pero, en cambio, el espacio de almacenamiento y el tiempo de carga de los datos en memoria serían más elevados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuración de ImageGenerator\n",
    "\n",
    "Cogiendo de base el modelo presentado en el capítulo anterior, apliquemos la técnica Data Augmentation pasando nuevos argumentos al objeto ImageGenerator de los datos de entrenamiento. Hay varios parámetros; nosotros hemos elegido los siguientes para nuestro caso de estudio: \n",
    "```Python\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ``rotation_range`` es un valor en grados (0-180) que indica el rango dentrodel cual se pueden rotar imágenes al azar.\n",
    "* ``width_shift`` y height_shift son rangos (como una fracción del ancho y laaltura total) dentro de los cuales se pueden trasladar las imágenes alazar verticalmente u horizontalmente.\n",
    "* shear_range sirve para aplicar transformaciones de corte al azar.\n",
    "* ``zoom_range sirve para aplicar zoom aleatorio dentro de las imágenes\n",
    "* ``horizontal_flip`` sirve para voltear aleatoriamente la mitad de las imágeneshorizontalmente (en nuestro caso de estudio no tiene sentido voltearverticalmente las imágenes).\n",
    "* ``fill_mode`` es la estrategia utilizada para rellenar los píxeles reciéncreados que pueden aparecer después de una de las transformaciones anteriores.\n",
    "\n",
    "Estas son solo algunas de las opciones disponibles de transformación. Todas las restantes se pueden consultar en la página web de la API preprocessing de Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4fc7c67a6ccb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from numpy import expand_dims\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from google.colab import files\n",
    "from tensorflow.keras.preprocessing import image\n",
    " \n",
    "uploaded=files.upload()\n",
    "for fn in uploaded.keys():\n",
    "    path='/content/' + fn\n",
    "    img=image.load_img(path)\n",
    "    data = img_to_array(img)\n",
    "    samples = expand_dims(data, 0)\n",
    "  \n",
    "    # example of \"rotation_range\"\n",
    "    datagen = ImageDataGenerator(rotation_range=45)\n",
    "\n",
    "    it = datagen.flow(samples, batch_size=1)\n",
    "    for i in range(6):\n",
    "        pyplot.subplot(230 + 1 + i)\n",
    "        batch = it.next()\n",
    "        image = batch[0].astype('uint8')\n",
    "        pyplot.imshow(image)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver cómo se han generado diferentes imágenes aleatoriamente a partir de la imagen original y en base al argumento rotation_range. Se puede probar por su cuenta otros argumentos usando este código como base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7j8fvlLyachD"
   },
   "source": [
    "## Carga de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6t5CmAYSvj-5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from google.colab import files\n",
    "\n",
    "# se debe cargar el fichero “cats_and_dogs_small.zip”\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJN-eZ0YBsYl"
   },
   "outputs": [],
   "source": [
    "local_zip = '/content/cats_and_dogs_small.zip'\n",
    "\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "\n",
    "zip_ref.extractall('/content')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZtsbWwXcBscw"
   },
   "outputs": [],
   "source": [
    "base_dir = '/content/cats_and_dogs_small'\n",
    "\n",
    "train_dir =      os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir =       os.path.join(base_dir, 'test')\n",
    "\n",
    "# Directorio con las imagenes de training \n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "# Directorio con las imagenes de validation\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "\n",
    "# Directorio con las imagenes de test\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "W9r7wCYmBsix",
    "outputId": "4edbf2ea-df14-446a-dff7-eb0e78e34478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images : 1000\n",
      "total training dog images : 1000\n",
      "total validation cat images : 500\n",
      "total validation dog images : 500\n",
      "total test cat images : 500\n",
      "total test dog images : 500\n"
     ]
    }
   ],
   "source": [
    "print('total training cat images :', len(os.listdir(train_cats_dir ) ))\n",
    "print('total training dog images :', len(os.listdir(train_dogs_dir ) ))\n",
    "\n",
    "print('total validation cat images :', len(os.listdir( validation_cats_dir ) ))\n",
    "print('total validation dog images :', len(os.listdir( validation_dogs_dir ) ))\n",
    "\n",
    "print('total test cat images :', len(os.listdir( test_cats_dir ) ))\n",
    "print('total test dog images :', len(os.listdir( test_dogs_dir ) ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aumentación de los datos\n",
    "\n",
    "Es importante resaltar que el aumento de datos de imagen generalmente solo se aplica al conjunto de datos de entrenamiento, y no al conjunto de datos de validación o prueba. Esto es diferente de la preparación de datos, como el cambio de tamaño de la imagen y la escala de píxeles, que sí se aplica a todos. Teniendo en cuenta esto, los generadores para nuestro caso de estudio se pueden especificar de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elvc8kFYKj0Y"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "validation_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
    "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    batch_size=20,\n",
    "                                                    class_mode='binary',\n",
    "                                                    target_size=(150, 150))     \n",
    "\n",
    "validation_generator =  validation_datagen.flow_from_directory(validation_dir,\n",
    "                                                         batch_size=20,\n",
    "                                                         class_mode  = 'binary',\n",
    "                                                         target_size = (150, 150))\n",
    "\n",
    "\n",
    "test_generator =  test_datagen.flow_from_directory(validation_dir,\n",
    "                                                         batch_size=20,\n",
    "                                                         class_mode  = 'binary',\n",
    "                                                         target_size = (150, 150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5RgKnvsfKkr8"
   },
   "source": [
    "## Creación del modelo CNN con DA: Modelo CNN con *Data Augmentation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de una red convolucional de más capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JaBxETn7Kjwj"
   },
   "outputs": [],
   "source": [
    "modelDA = Sequential()\n",
    "modelDA.add(Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)))\n",
    "modelDA.add(MaxPooling2D(2, 2))\n",
    "modelDA.add(Conv2D(64, (3,3), activation='relu'))\n",
    "modelDA.add(MaxPooling2D(2,2))\n",
    "modelDA.add(Conv2D(128, (3,3), activation='relu'))\n",
    "modelDA.add(MaxPooling2D(2,2))\n",
    "modelDA.add(Conv2D(128, (3,3), activation='relu'))\n",
    "modelDA.add(MaxPooling2D(2,2))\n",
    "modelDA.add(Flatten())\n",
    "modelDA.add(Dense(512, activation='relu'))\n",
    "modelDA.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobar la estructura de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDA.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDA.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la red\n",
    "\n",
    "Ahora que ya se ha definido la estructura de red, el optimizador, learning rate y metricas ya se puede entrenar la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xU48fLeFKj3I",
    "outputId": "82c9d71d-2b37-4ffb-e1c9-adf7b02f9ed1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 100 steps, validate for 50 steps\n",
      "Epoch 1/100\n",
      "100/100 - 22s - loss: 0.6915 - acc: 0.5285 - val_loss: 0.6991 - val_acc: 0.5010\n",
      "Epoch 2/100\n",
      "100/100 - 16s - loss: 0.6760 - acc: 0.5715 - val_loss: 0.6571 - val_acc: 0.5930\n",
      "Epoch 3/100\n",
      "100/100 - 16s - loss: 0.6704 - acc: 0.5840 - val_loss: 0.6462 - val_acc: 0.6150\n",
      "Epoch 4/100\n",
      "100/100 - 16s - loss: 0.6551 - acc: 0.6065 - val_loss: 0.6457 - val_acc: 0.6080\n",
      "Epoch 5/100\n",
      "100/100 - 16s - loss: 0.6427 - acc: 0.6350 - val_loss: 0.6186 - val_acc: 0.6410\n",
      "Epoch 6/100\n",
      "100/100 - 16s - loss: 0.6161 - acc: 0.6550 - val_loss: 0.7826 - val_acc: 0.5400\n",
      "Epoch 7/100\n",
      "100/100 - 16s - loss: 0.6153 - acc: 0.6600 - val_loss: 0.6238 - val_acc: 0.6460\n",
      "Epoch 8/100\n",
      "100/100 - 16s - loss: 0.6060 - acc: 0.6720 - val_loss: 0.6084 - val_acc: 0.6520\n",
      "Epoch 9/100\n",
      "100/100 - 16s - loss: 0.6054 - acc: 0.6665 - val_loss: 0.5618 - val_acc: 0.7060\n",
      "Epoch 10/100\n",
      "100/100 - 16s - loss: 0.5962 - acc: 0.6740 - val_loss: 0.5512 - val_acc: 0.7010\n",
      "Epoch 11/100\n",
      "100/100 - 16s - loss: 0.5884 - acc: 0.6820 - val_loss: 0.5690 - val_acc: 0.6970\n",
      "Epoch 12/100\n",
      "100/100 - 16s - loss: 0.5833 - acc: 0.6860 - val_loss: 0.5456 - val_acc: 0.7140\n",
      "Epoch 13/100\n",
      "100/100 - 16s - loss: 0.5760 - acc: 0.7045 - val_loss: 0.5384 - val_acc: 0.7230\n",
      "Epoch 14/100\n",
      "100/100 - 16s - loss: 0.5718 - acc: 0.7025 - val_loss: 0.5408 - val_acc: 0.7130\n",
      "Epoch 15/100\n",
      "100/100 - 16s - loss: 0.5644 - acc: 0.7120 - val_loss: 0.5487 - val_acc: 0.7130\n",
      "Epoch 16/100\n",
      "100/100 - 16s - loss: 0.5522 - acc: 0.7165 - val_loss: 0.5362 - val_acc: 0.7220\n",
      "Epoch 17/100\n",
      "100/100 - 16s - loss: 0.5492 - acc: 0.7190 - val_loss: 0.5185 - val_acc: 0.7340\n",
      "Epoch 18/100\n",
      "100/100 - 16s - loss: 0.5498 - acc: 0.7190 - val_loss: 0.5279 - val_acc: 0.7350\n",
      "Epoch 19/100\n",
      "100/100 - 16s - loss: 0.5462 - acc: 0.7175 - val_loss: 0.5146 - val_acc: 0.7340\n",
      "Epoch 20/100\n",
      "100/100 - 16s - loss: 0.5299 - acc: 0.7375 - val_loss: 0.5023 - val_acc: 0.7490\n",
      "Epoch 21/100\n",
      "100/100 - 16s - loss: 0.5369 - acc: 0.7280 - val_loss: 0.4990 - val_acc: 0.7510\n",
      "Epoch 22/100\n",
      "100/100 - 16s - loss: 0.5443 - acc: 0.7245 - val_loss: 0.5361 - val_acc: 0.7380\n",
      "Epoch 23/100\n",
      "100/100 - 16s - loss: 0.5296 - acc: 0.7350 - val_loss: 0.5000 - val_acc: 0.7480\n",
      "Epoch 24/100\n",
      "100/100 - 16s - loss: 0.5215 - acc: 0.7405 - val_loss: 0.5027 - val_acc: 0.7320\n",
      "Epoch 25/100\n",
      "100/100 - 16s - loss: 0.5282 - acc: 0.7385 - val_loss: 0.5302 - val_acc: 0.7230\n",
      "Epoch 26/100\n",
      "100/100 - 16s - loss: 0.5271 - acc: 0.7325 - val_loss: 0.4907 - val_acc: 0.7460\n",
      "Epoch 27/100\n",
      "100/100 - 16s - loss: 0.5150 - acc: 0.7475 - val_loss: 0.5392 - val_acc: 0.7330\n",
      "Epoch 28/100\n",
      "100/100 - 16s - loss: 0.5023 - acc: 0.7510 - val_loss: 0.4819 - val_acc: 0.7580\n",
      "Epoch 29/100\n",
      "100/100 - 16s - loss: 0.4841 - acc: 0.7635 - val_loss: 0.4777 - val_acc: 0.7630\n",
      "Epoch 30/100\n",
      "100/100 - 16s - loss: 0.5061 - acc: 0.7615 - val_loss: 0.4956 - val_acc: 0.7590\n",
      "Epoch 31/100\n",
      "100/100 - 16s - loss: 0.5057 - acc: 0.7485 - val_loss: 0.4807 - val_acc: 0.7600\n",
      "Epoch 32/100\n",
      "100/100 - 16s - loss: 0.4914 - acc: 0.7605 - val_loss: 0.4784 - val_acc: 0.7620\n",
      "Epoch 33/100\n",
      "100/100 - 16s - loss: 0.5027 - acc: 0.7520 - val_loss: 0.4734 - val_acc: 0.7710\n",
      "Epoch 34/100\n",
      "100/100 - 16s - loss: 0.4856 - acc: 0.7625 - val_loss: 0.5013 - val_acc: 0.7520\n",
      "Epoch 35/100\n",
      "100/100 - 16s - loss: 0.4897 - acc: 0.7700 - val_loss: 0.4638 - val_acc: 0.7740\n",
      "Epoch 36/100\n",
      "100/100 - 16s - loss: 0.4787 - acc: 0.7600 - val_loss: 0.5560 - val_acc: 0.7130\n",
      "Epoch 37/100\n",
      "100/100 - 16s - loss: 0.4950 - acc: 0.7585 - val_loss: 0.4735 - val_acc: 0.7610\n",
      "Epoch 38/100\n",
      "100/100 - 16s - loss: 0.4805 - acc: 0.7635 - val_loss: 0.4724 - val_acc: 0.7640\n",
      "Epoch 39/100\n",
      "100/100 - 16s - loss: 0.4797 - acc: 0.7610 - val_loss: 0.4679 - val_acc: 0.7730\n",
      "Epoch 40/100\n",
      "100/100 - 16s - loss: 0.4838 - acc: 0.7580 - val_loss: 0.4661 - val_acc: 0.7720\n",
      "Epoch 41/100\n",
      "100/100 - 16s - loss: 0.4823 - acc: 0.7680 - val_loss: 0.4862 - val_acc: 0.7650\n",
      "Epoch 42/100\n",
      "100/100 - 16s - loss: 0.4742 - acc: 0.7755 - val_loss: 0.4690 - val_acc: 0.7610\n",
      "Epoch 43/100\n",
      "100/100 - 16s - loss: 0.4694 - acc: 0.7745 - val_loss: 0.5381 - val_acc: 0.7360\n",
      "Epoch 44/100\n",
      "100/100 - 16s - loss: 0.4645 - acc: 0.7795 - val_loss: 0.4587 - val_acc: 0.7710\n",
      "Epoch 45/100\n",
      "100/100 - 16s - loss: 0.4608 - acc: 0.7740 - val_loss: 0.4535 - val_acc: 0.7830\n",
      "Epoch 46/100\n",
      "100/100 - 16s - loss: 0.4621 - acc: 0.7755 - val_loss: 0.4627 - val_acc: 0.7810\n",
      "Epoch 47/100\n",
      "100/100 - 16s - loss: 0.4658 - acc: 0.7715 - val_loss: 0.4454 - val_acc: 0.7950\n",
      "Epoch 48/100\n",
      "100/100 - 16s - loss: 0.4723 - acc: 0.7760 - val_loss: 0.5411 - val_acc: 0.7240\n",
      "Epoch 49/100\n",
      "100/100 - 16s - loss: 0.4598 - acc: 0.7850 - val_loss: 0.5247 - val_acc: 0.7430\n",
      "Epoch 50/100\n",
      "100/100 - 16s - loss: 0.4623 - acc: 0.7790 - val_loss: 0.4842 - val_acc: 0.7660\n",
      "Epoch 51/100\n",
      "100/100 - 16s - loss: 0.4497 - acc: 0.7870 - val_loss: 0.5380 - val_acc: 0.7400\n",
      "Epoch 52/100\n",
      "100/100 - 16s - loss: 0.4568 - acc: 0.7760 - val_loss: 0.4641 - val_acc: 0.7660\n",
      "Epoch 53/100\n",
      "100/100 - 16s - loss: 0.4505 - acc: 0.7915 - val_loss: 0.5178 - val_acc: 0.7540\n",
      "Epoch 54/100\n",
      "100/100 - 16s - loss: 0.4384 - acc: 0.7920 - val_loss: 0.4502 - val_acc: 0.7910\n",
      "Epoch 55/100\n",
      "100/100 - 16s - loss: 0.4423 - acc: 0.7960 - val_loss: 0.4578 - val_acc: 0.7860\n",
      "Epoch 56/100\n",
      "100/100 - 16s - loss: 0.4401 - acc: 0.7995 - val_loss: 0.4677 - val_acc: 0.7730\n",
      "Epoch 57/100\n",
      "100/100 - 16s - loss: 0.4220 - acc: 0.8080 - val_loss: 0.5128 - val_acc: 0.7590\n",
      "Epoch 58/100\n",
      "100/100 - 16s - loss: 0.4156 - acc: 0.8115 - val_loss: 0.4617 - val_acc: 0.7860\n",
      "Epoch 59/100\n",
      "100/100 - 16s - loss: 0.4313 - acc: 0.7925 - val_loss: 0.4384 - val_acc: 0.8010\n",
      "Epoch 60/100\n",
      "100/100 - 16s - loss: 0.4496 - acc: 0.7960 - val_loss: 0.4771 - val_acc: 0.7750\n",
      "Epoch 61/100\n",
      "100/100 - 16s - loss: 0.4243 - acc: 0.7990 - val_loss: 0.7587 - val_acc: 0.6820\n",
      "Epoch 62/100\n",
      "100/100 - 16s - loss: 0.4192 - acc: 0.8040 - val_loss: 0.4456 - val_acc: 0.7860\n",
      "Epoch 63/100\n",
      "100/100 - 16s - loss: 0.4191 - acc: 0.8060 - val_loss: 0.4538 - val_acc: 0.7880\n",
      "Epoch 64/100\n",
      "100/100 - 16s - loss: 0.4218 - acc: 0.8140 - val_loss: 0.4516 - val_acc: 0.7890\n",
      "Epoch 65/100\n",
      "100/100 - 16s - loss: 0.4266 - acc: 0.8085 - val_loss: 0.4404 - val_acc: 0.8040\n",
      "Epoch 66/100\n",
      "100/100 - 16s - loss: 0.4189 - acc: 0.7985 - val_loss: 0.4708 - val_acc: 0.7840\n",
      "Epoch 67/100\n",
      "100/100 - 16s - loss: 0.4252 - acc: 0.7950 - val_loss: 0.4745 - val_acc: 0.7880\n",
      "Epoch 68/100\n",
      "100/100 - 16s - loss: 0.4171 - acc: 0.8120 - val_loss: 0.4266 - val_acc: 0.7960\n",
      "Epoch 69/100\n",
      "100/100 - 16s - loss: 0.4101 - acc: 0.8095 - val_loss: 0.4685 - val_acc: 0.7920\n",
      "Epoch 70/100\n",
      "100/100 - 16s - loss: 0.4062 - acc: 0.8060 - val_loss: 0.4426 - val_acc: 0.8010\n",
      "Epoch 71/100\n",
      "100/100 - 16s - loss: 0.3999 - acc: 0.8135 - val_loss: 0.4500 - val_acc: 0.7870\n",
      "Epoch 72/100\n",
      "100/100 - 16s - loss: 0.4123 - acc: 0.8115 - val_loss: 0.5149 - val_acc: 0.7690\n",
      "Epoch 73/100\n",
      "100/100 - 16s - loss: 0.4087 - acc: 0.8120 - val_loss: 0.4487 - val_acc: 0.8040\n",
      "Epoch 74/100\n",
      "100/100 - 16s - loss: 0.3968 - acc: 0.8165 - val_loss: 0.5558 - val_acc: 0.7330\n",
      "Epoch 75/100\n",
      "100/100 - 16s - loss: 0.4136 - acc: 0.8080 - val_loss: 0.4362 - val_acc: 0.7980\n",
      "Epoch 76/100\n",
      "100/100 - 16s - loss: 0.3993 - acc: 0.8185 - val_loss: 0.4781 - val_acc: 0.7750\n",
      "Epoch 77/100\n",
      "100/100 - 16s - loss: 0.3955 - acc: 0.8205 - val_loss: 0.4283 - val_acc: 0.8080\n",
      "Epoch 78/100\n",
      "100/100 - 16s - loss: 0.3867 - acc: 0.8270 - val_loss: 0.4402 - val_acc: 0.8110\n",
      "Epoch 79/100\n",
      "100/100 - 16s - loss: 0.3888 - acc: 0.8195 - val_loss: 0.6029 - val_acc: 0.7340\n",
      "Epoch 80/100\n",
      "100/100 - 16s - loss: 0.3992 - acc: 0.8125 - val_loss: 0.4441 - val_acc: 0.7880\n",
      "Epoch 81/100\n",
      "100/100 - 16s - loss: 0.3890 - acc: 0.8265 - val_loss: 0.5065 - val_acc: 0.7810\n",
      "Epoch 82/100\n",
      "100/100 - 16s - loss: 0.3919 - acc: 0.8280 - val_loss: 0.4123 - val_acc: 0.8140\n",
      "Epoch 83/100\n",
      "100/100 - 16s - loss: 0.3874 - acc: 0.8165 - val_loss: 0.4390 - val_acc: 0.8000\n",
      "Epoch 84/100\n",
      "100/100 - 16s - loss: 0.3867 - acc: 0.8250 - val_loss: 0.4831 - val_acc: 0.7840\n",
      "Epoch 85/100\n",
      "100/100 - 16s - loss: 0.3825 - acc: 0.8310 - val_loss: 0.4292 - val_acc: 0.8110\n",
      "Epoch 86/100\n",
      "100/100 - 16s - loss: 0.3616 - acc: 0.8310 - val_loss: 0.4538 - val_acc: 0.8020\n",
      "Epoch 87/100\n",
      "100/100 - 16s - loss: 0.3917 - acc: 0.8270 - val_loss: 0.4752 - val_acc: 0.7960\n",
      "Epoch 88/100\n",
      "100/100 - 16s - loss: 0.3697 - acc: 0.8360 - val_loss: 0.4652 - val_acc: 0.7950\n",
      "Epoch 89/100\n",
      "100/100 - 16s - loss: 0.3593 - acc: 0.8330 - val_loss: 0.5362 - val_acc: 0.7600\n",
      "Epoch 90/100\n",
      "100/100 - 16s - loss: 0.3656 - acc: 0.8345 - val_loss: 0.4996 - val_acc: 0.7920\n",
      "Epoch 91/100\n",
      "100/100 - 16s - loss: 0.3716 - acc: 0.8215 - val_loss: 0.4642 - val_acc: 0.8030\n",
      "Epoch 92/100\n",
      "100/100 - 16s - loss: 0.3698 - acc: 0.8365 - val_loss: 0.4574 - val_acc: 0.8020\n",
      "Epoch 93/100\n",
      "100/100 - 16s - loss: 0.3713 - acc: 0.8415 - val_loss: 0.4445 - val_acc: 0.8100\n",
      "Epoch 94/100\n",
      "100/100 - 16s - loss: 0.3574 - acc: 0.8315 - val_loss: 0.6288 - val_acc: 0.7500\n",
      "Epoch 95/100\n",
      "100/100 - 16s - loss: 0.3613 - acc: 0.8365 - val_loss: 0.4245 - val_acc: 0.8120\n",
      "Epoch 96/100\n",
      "100/100 - 16s - loss: 0.3599 - acc: 0.8380 - val_loss: 0.4430 - val_acc: 0.8190\n",
      "Epoch 97/100\n",
      "100/100 - 16s - loss: 0.3607 - acc: 0.8385 - val_loss: 0.4906 - val_acc: 0.7710\n",
      "Epoch 98/100\n",
      "100/100 - 16s - loss: 0.3646 - acc: 0.8320 - val_loss: 0.4369 - val_acc: 0.8100\n",
      "Epoch 99/100\n",
      "100/100 - 16s - loss: 0.3548 - acc: 0.8415 - val_loss: 0.4510 - val_acc: 0.7920\n",
      "Epoch 100/100\n",
      "100/100 - 16s - loss: 0.3502 - acc: 0.8435 - val_loss: 0.5326 - val_acc: 0.7840\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "steps_per_epoch = train_generator.n // batch_size\n",
    "validation_steps = validation_generator.n // batch_size\n",
    "\n",
    "historyDA = modelDA.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch= steps_per_epoch,\n",
    "    epochs= 100,\n",
    "    validation_data= validation_generator,\n",
    "    validation_steps= validation_steps,\n",
    "    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de la red\n",
    "\n",
    "Para ver si la red ha aprendido a clasificar bien los números manuscritos hay que comprobar la precisión que obtiene en el conjunto de datos de test. Para este conjunto no ha sido entrenada la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "zkmHqrzCKj8g",
    "outputId": "1b91ca94-1eda-4a1f-e962-d3a3f0369d73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "50\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "50/50 [==============================] - 3s 54ms/step - loss: 0.5326 - acc: 0.7840\n",
      "Test Accuracy: 0.784\n"
     ]
    }
   ],
   "source": [
    "print (steps_per_epoch)\n",
    "print (validation_steps)\n",
    "test_lost, test_acc= modelDA.evaluate(test_generator)\n",
    "print (\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "colab_type": "code",
    "id": "GP2sz8eoKj_T",
    "outputId": "09a9ad92-2b5b-4f3e-a9de-83986a5b1001"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERERtZIgIiKi1n8BA+8f+SDIougAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc      = historyDA.history['acc' ]\n",
    "val_acc  = historyDA.history['val_acc']\n",
    "loss     = historyDA.history['loss' ]\n",
    "val_loss = historyDA.history['val_loss']\n",
    "\n",
    "epochs    = range(1,len(acc)+1,1) # obtener número de epochs\n",
    "\n",
    "plt.plot  ( epochs,     acc, 'r--', label='Training acc'  )\n",
    "plt.plot  ( epochs, val_acc,  'b', label='Validation acc')\n",
    "plt.title ('Training and validation accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.plot  ( epochs,     loss, 'r--' )\n",
    "plt.plot  ( epochs, val_loss ,  'b' )\n",
    "plt.title ('Training and validation loss'   )\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En resumen, en esta sección hemos examinado esta técnica realmente útil que la API Keras ofrece para simular efectivamente un conjunto de datos más grande para entrenar la red neuronal y poder reducir así el problema de sobreentrenamiento.\n",
    "Aun así, las entradas que ve la red neuronal todavía están muy correlacionadas, ya que provienen de una pequeña cantidad de imágenes originales: no puede producir nueva información, solo puede mezclar la información existente. Como tal, esto puede no ser suficiente para deshacerse por completo del overfitting en algunos casos. Para seguir luchando en la mitigación del sobreentrenamiento se pueden añadir técnicas como la regularización o Dropout, que hemos comentado anteriormente. Pero incluso así, a veces, debido a no tener más datos, no es suficiente para obtener mejores modelos. Para estos casos aún nos queda otra oportunidad, que presentamos en la siguiente sección: usar modelos preentrenados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Se evidenció cómo aminorar el efecto de un sobreajuste de un modelo con una técnica muy útil para imágenes, como es Data Augmentation. Pero, en realidad, nos encontramos limitados en cuanto a descubrir características de los datos, puesto que si los datos de entrenamiento son muy pocos, al hacer transformaciones con la técnica de Data Augmentation parte de las características de las nuevas imágenes son repeticiones de las anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepto de Transfer Learning\n",
    "\n",
    "Transfer Learning es una de las técnicas centrales actualmente en Deep Learning. Su interés radica en que, en lugar de necesitar entrenar una red neuronal desde cero, lo cual implica disponer de una gran cantidad de datos y de mucho tiempo (días o semanas) de computación para entrenar, lo hacemos desde una red preentrenada.\n",
    "\n",
    "Esta técnica nos permite descargar un modelo de código abierto que alguien ya ha entrenado previamente en un gran conjunto de datos y usar sus parámetros (miles o millones) como punto de partida para, posteriormente, continuar entrenando el modelo con el conjunto de datos (más pequeño) que tengamos para una tarea determinada.\n",
    "\n",
    "Si el conjunto de datos original con el que se entrenó la red neuronal preentrenada es suficientemente grande y general, entonces la jerarquía espacial de las características (features) aprendidas por la red preentrenada permite al modelo preentrenado actuar como un modelo genérico del mundo visual y, por lo tanto, sus características pueden resultar útiles para muchos problemas diferentes de visión por computadora, a pesar de que estos nuevos problemas pueden involucrar clases completamente diferentes a las de la tarea original\n",
    "\n",
    "Recordaremos que en una red neuronal convolucional cada capa va aprendiendo diferentes niveles de abstracción, siendo las primeras capas las encargadas de aprender características más genéricas. Estas primeras capas son las que podemos volver a usar fácilmente, puesto que las características aprendidas son aplicables a otros problemas.\n",
    "\n",
    "Esta «portabilidad» de las características aprendidas a través de diferentes problemas es una de las virtudes clave del Transfer Learning. Específicamente, en el caso de la visión por computador, muchos modelos previamente entrenados (muchos entrenados en el conjunto de datos ImageNet ahora están disponibles públicamente para su descarga y se pueden usar para crear potentes modelos de visión con muy pocos datos.\n",
    "\n",
    "Hay dos formas de utilizar una red preentrenada, que presentamos a continuación: Feature Extraction y Fine-Tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Feature Extraction consiste en utilizar los parámetros aprendidos por una red preentrenada para extraer características (features) interesantes de nuevos datos.\n",
    "Estas características obtenidas de los nuevos datos se procesan a través de un nuevo clasificador, que se entrena desde cero.\n",
    "\n",
    "Las ConvNet utilizadas para clasificar imágenes se componían de dos partes: una serie de capas de convoluciones y pooling, y un clasificador formado habitualmente por una o varias capas densas.\n",
    "\n",
    "La primera parte se llama base convolucional (convolutional base) del modelo. En el caso de las ConvNet, la extracción de características consiste en tomar la base convolucional de una red previamente entrenada, ejecutar los nuevos datos a través de ella y entrenar a un nuevo clasificador en la parte de la salida.\n",
    "\n",
    "En la Figura 11.4 se muestra de forma esquemática esta idea. A la izquierda se muestra una ConvNet resaltando sus dos partes, la compuesta por las capas convolucionales y de pooling, y el clasificador final. En la figura del medio resaltamos que de esta ConvNet de base desechamos las capas finales de clasificación y nos quedamos con la primera parte, tanto con su estructura de capas como con los valores de los parámetros (pesos y sesgos de sus neuronas), que llamamos base convolucional. Finalmente, en la figura de la derecha se representa la estructura de la ConvNet que usaremos para clasificar nuestros nuevos datos. A la base convolucional que ya se encuentra entrenada le añadimos las capas finales que se requieran para hacer el clasificador a la medida del problema que estamos tratando. Recordemos que este nuevo clasificador sí que requiere entrenamiento\n",
    "\n",
    "Representación esquemática de las redes neuronales preentrenadas usadas en la técnica de Feature Extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulta muy útil reutilizar la base convolucional porque las características aprendidas por la red preeentrenada de donde se ha extraído son mapas de características de conceptos genéricos sobre una imagen, lo que en general resulta «portable» a otros problemas del mismo ámbito. En general no se reutiliza el clasificador entrenado, ya que las representaciones aprendidas por el clasificador serán necesariamente específicas del conjunto de clases en las que se formó el modelo; solo contendrán información sobre la probabilidad de presencia de esta o aquella clase en la imagen completa.\n",
    "\n",
    "Además, las representaciones encontradas en capas densamente conectadas ya no mantienen información sobre dónde se encuentran los objetos en la imagen de entrada porque estas capas eliminan la noción de espacio, a diferencia de las capas convolucionales\n",
    "\n",
    "Lo más interesante para nosotros ahora es que la API de Keras nos permite aplicar esta técnica de una manera muy fácil y con pocas líneas de código.\n",
    "Keras permite descargar un modelo y luego configurar cómo este debe ser entrenado, indicando qué capas son entrenables (Trainable layers) y qué capas no (Frozen layers).\n",
    "\n",
    "En general se requieren unas cuantas iteraciones de prueba y error para descubrir la combinación correcta. Concretamente en el caso de visión por computador, muchos modelos preentrenados con ImageNet se encuentran disponibles en la mayoría de librerías de desarrollo de Deep Learning, como es el caso de Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En concreto en la página de modelos preentrenados ``tf.keras.applications172`` podemos encontrar disponibles los siguientes modelos para clasificación de imágenes entrenados con ImageNet:\n",
    "* Xception\n",
    "* VGG16\n",
    "* VGG19\n",
    "* ResNet50\n",
    "* InceptionV3\n",
    "* InceptionResNetV2\n",
    "* MobileNet\n",
    "* MobileNetV2\n",
    "* DenseNet\n",
    "* NASNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccc5u4fembNS"
   },
   "source": [
    "###  VGG16: Modelo con *Feature Extraction*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se usará la red neuronal VGG16, que tiene una arquitectura convolucional como las que hemos comentado en los capítulos previos.\n",
    "La red neuronal VGG16 se ha entrenado previamente en un conjunto de datos de ImageNet, que recordemos que tiene 1.4 millones de imágenes en 1000 clases diferentes.\n",
    "ImageNet contiene muchas clases de animales, incluidas diferentes especies de gatos y perros, por lo que podemos esperar un buen rendimiento en el problema de clasificación de perros contra gatos.\n",
    "El modelo VGG16 es una red de 16 capas propuesta por Karen Simonyan y Andrew Zisserman en su artículo «Very Deep Convolutional Networks for Large-Scale Image Recognition»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo VGG16 podemos importarlo desde el módulo ``keras.applications``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A este constructor de la red VGG16 se le pasan tres argumentos.\n",
    "1. La forma de los tensores de imágenes que alimentarán a la red (es un argumento opcional).\n",
    "2. El argumento ``include_top`` donde se indica si se debe incluir (o no) el clasificador en la última capa de la red. \n",
    "    * Este caso, esta capa corresponde a la capa que clasifica las 1000 clases de ImageNet. Debido a que tenemos la intención de usar nuestro propio clasificador (que clasifica solo dos clases: gato y perro) no necesitamos incluirla.\n",
    "3. El último argumento, ``weights``, que indica de dónde se obtiene la información para iniciar los pesos de los parámetros de la red (en nuestro caso usaremos ImageNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "V2uilIo5hY4y",
    "outputId": "f24bf48c-9e28-404f-c322-41a8d6dcf4b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 2s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_trained_model = VGG16(input_shape = (150, 150, 3), \n",
    "                                include_top = False, \n",
    "                                weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el método ``summary()`` podemos saber el detalle de la arquitectura de la base convolucional VGG16:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si comparamos con la red neuronal VGG16 completa, se puede comprobar que no existen las dos capas finales fully-connected de 4096 neuronas ni la de 1000 neuronas (cada una de estas neuronas representa una de las categorías de imágenes de ImageNet), las cuales no incluten con el valor **``False``** del argumento ``include_top``\n",
    "\n",
    "No se necesitan las últimas capas (que son el clasificador de VGG16) porque se crearán unas propias para construir un clasificador y predecir si las imágenes serán un perro o un gato a partir del último feature map de forma y tamaño (4, 4, 512) que nos devuelve la base convolucional VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importante: Antes de entrenar el modelo es necesario indicar cual de las capas de la base convolucional no deben ser entrenadas, lo que se denomina «congelar» capas (Freeze layers).\n",
    "\n",
    "Congelar una capa o un conjunto de capas significa evitar que sus pesos se actualicen durante el entrenamiento.\n",
    "* Si no se hiciera esto, los valores que fueron previamente aprendidos por la base convolucional serían modificados durante el entrenamiento, efecto que no se busca.\n",
    "* En Keras, congelar una capa se realiza estableciendo su atributo trainable a False:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "pre_trained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En Keras los modelos los podemos considerar como capas y, por tanto, podemos agregar un modelo (como pre_trained_model) a un modelo secuencial al igual que agregaríamos una capa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LSbSk4CKd4ev"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFE = Sequential()\n",
    "modelFE.add(pre_trained_model)\n",
    "modelFE.add(Flatten())\n",
    "modelFE.add(Dense(256, activation='relu'))\n",
    "modelFE.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así es como el método summary() presenta la red neuronal que se acaba de construir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "p1KRETLXuHpZ",
    "outputId": "22ba0523-964b-446d-b92d-a6e71348524c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               2097408   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 16,812,353\n",
      "Trainable params: 2,097,665\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelFE.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos el preprocesado de **Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "InQuF32GiBRP",
    "outputId": "281d6e22-3a80-40ad-b00a-7c8b9c14a604"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "validation_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
    "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    batch_size=20,\n",
    "                                                    class_mode='binary',\n",
    "                                                    target_size=(150, 150))     \n",
    "\n",
    "validation_generator =  validation_datagen.flow_from_directory(validation_dir,\n",
    "                                                         batch_size=20,\n",
    "                                                         class_mode  = 'binary',\n",
    "                                                         target_size = (150, 150))\n",
    "\n",
    "test_generator =  test_datagen.flow_from_directory(validation_dir,\n",
    "                                                         batch_size=20,\n",
    "                                                         class_mode  = 'binary',\n",
    "                                                         target_size = (150, 150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se compila red utilizando los mismos parametros anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "DCSzluJsiBXA",
    "outputId": "144b618b-3155-47d4-d24c-0cc0364246ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 100 steps, validate for 50 steps\n",
      "Epoch 1/100\n",
      "100/100 - 18s - loss: 0.5224 - acc: 0.7305 - val_loss: 0.3127 - val_acc: 0.8760\n",
      "Epoch 2/100\n",
      "100/100 - 17s - loss: 0.4142 - acc: 0.8045 - val_loss: 0.3264 - val_acc: 0.8460\n",
      "Epoch 3/100\n",
      "100/100 - 17s - loss: 0.3801 - acc: 0.8285 - val_loss: 0.2664 - val_acc: 0.8930\n",
      "Epoch 4/100\n",
      "100/100 - 16s - loss: 0.3718 - acc: 0.8265 - val_loss: 0.3015 - val_acc: 0.8660\n",
      "Epoch 5/100\n",
      "100/100 - 17s - loss: 0.3528 - acc: 0.8425 - val_loss: 0.2664 - val_acc: 0.8900\n",
      "Epoch 6/100\n",
      "100/100 - 17s - loss: 0.3309 - acc: 0.8490 - val_loss: 0.2611 - val_acc: 0.8930\n",
      "Epoch 7/100\n",
      "100/100 - 17s - loss: 0.3309 - acc: 0.8470 - val_loss: 0.2519 - val_acc: 0.9000\n",
      "Epoch 8/100\n",
      "100/100 - 17s - loss: 0.3192 - acc: 0.8510 - val_loss: 0.2597 - val_acc: 0.8990\n",
      "Epoch 9/100\n",
      "100/100 - 16s - loss: 0.3211 - acc: 0.8610 - val_loss: 0.2968 - val_acc: 0.8820\n",
      "Epoch 10/100\n",
      "100/100 - 17s - loss: 0.3182 - acc: 0.8520 - val_loss: 0.2802 - val_acc: 0.8770\n",
      "Epoch 11/100\n",
      "100/100 - 17s - loss: 0.3069 - acc: 0.8665 - val_loss: 0.2508 - val_acc: 0.9000\n",
      "Epoch 12/100\n",
      "100/100 - 16s - loss: 0.3050 - acc: 0.8570 - val_loss: 0.2420 - val_acc: 0.9020\n",
      "Epoch 13/100\n",
      "100/100 - 17s - loss: 0.2890 - acc: 0.8735 - val_loss: 0.2642 - val_acc: 0.8990\n",
      "Epoch 14/100\n",
      "100/100 - 17s - loss: 0.2938 - acc: 0.8690 - val_loss: 0.2388 - val_acc: 0.9110\n",
      "Epoch 15/100\n",
      "100/100 - 17s - loss: 0.2969 - acc: 0.8545 - val_loss: 0.2383 - val_acc: 0.9100\n",
      "Epoch 16/100\n",
      "100/100 - 16s - loss: 0.2899 - acc: 0.8740 - val_loss: 0.2494 - val_acc: 0.8970\n",
      "Epoch 17/100\n",
      "100/100 - 17s - loss: 0.2925 - acc: 0.8690 - val_loss: 0.2448 - val_acc: 0.8970\n",
      "Epoch 18/100\n",
      "100/100 - 17s - loss: 0.2780 - acc: 0.8790 - val_loss: 0.2466 - val_acc: 0.8990\n",
      "Epoch 19/100\n",
      "100/100 - 17s - loss: 0.2762 - acc: 0.8770 - val_loss: 0.2463 - val_acc: 0.9050\n",
      "Epoch 20/100\n",
      "100/100 - 17s - loss: 0.2725 - acc: 0.8815 - val_loss: 0.2621 - val_acc: 0.8940\n",
      "Epoch 21/100\n",
      "100/100 - 17s - loss: 0.2764 - acc: 0.8835 - val_loss: 0.2422 - val_acc: 0.9080\n",
      "Epoch 22/100\n",
      "100/100 - 17s - loss: 0.2819 - acc: 0.8740 - val_loss: 0.2406 - val_acc: 0.9040\n",
      "Epoch 23/100\n",
      "100/100 - 16s - loss: 0.2694 - acc: 0.8795 - val_loss: 0.2961 - val_acc: 0.8810\n",
      "Epoch 24/100\n",
      "100/100 - 17s - loss: 0.2659 - acc: 0.8890 - val_loss: 0.2355 - val_acc: 0.9050\n",
      "Epoch 25/100\n",
      "100/100 - 17s - loss: 0.2626 - acc: 0.8895 - val_loss: 0.2456 - val_acc: 0.9080\n",
      "Epoch 26/100\n",
      "100/100 - 16s - loss: 0.2731 - acc: 0.8810 - val_loss: 0.2455 - val_acc: 0.9020\n",
      "Epoch 27/100\n",
      "100/100 - 17s - loss: 0.2747 - acc: 0.8735 - val_loss: 0.2460 - val_acc: 0.9050\n",
      "Epoch 28/100\n",
      "100/100 - 17s - loss: 0.2494 - acc: 0.8955 - val_loss: 0.3223 - val_acc: 0.8730\n",
      "Epoch 29/100\n",
      "100/100 - 17s - loss: 0.2479 - acc: 0.8945 - val_loss: 0.2523 - val_acc: 0.9000\n",
      "Epoch 30/100\n",
      "100/100 - 17s - loss: 0.2500 - acc: 0.8915 - val_loss: 0.2591 - val_acc: 0.9010\n",
      "Epoch 31/100\n",
      "100/100 - 17s - loss: 0.2456 - acc: 0.8955 - val_loss: 0.3038 - val_acc: 0.8800\n",
      "Epoch 32/100\n",
      "100/100 - 16s - loss: 0.2546 - acc: 0.8930 - val_loss: 0.2566 - val_acc: 0.8910\n",
      "Epoch 33/100\n",
      "100/100 - 17s - loss: 0.2311 - acc: 0.9045 - val_loss: 0.2710 - val_acc: 0.8930\n",
      "Epoch 34/100\n",
      "100/100 - 17s - loss: 0.2561 - acc: 0.8940 - val_loss: 0.2574 - val_acc: 0.9010\n",
      "Epoch 35/100\n",
      "100/100 - 17s - loss: 0.2511 - acc: 0.8930 - val_loss: 0.2481 - val_acc: 0.9060\n",
      "Epoch 36/100\n",
      "100/100 - 17s - loss: 0.2480 - acc: 0.8920 - val_loss: 0.2529 - val_acc: 0.9070\n",
      "Epoch 37/100\n",
      "100/100 - 17s - loss: 0.2393 - acc: 0.8920 - val_loss: 0.2538 - val_acc: 0.9110\n",
      "Epoch 38/100\n",
      "100/100 - 17s - loss: 0.2293 - acc: 0.8995 - val_loss: 0.2819 - val_acc: 0.8850\n",
      "Epoch 39/100\n",
      "100/100 - 17s - loss: 0.2378 - acc: 0.8960 - val_loss: 0.2818 - val_acc: 0.8870\n",
      "Epoch 40/100\n",
      "100/100 - 17s - loss: 0.2163 - acc: 0.9115 - val_loss: 0.2664 - val_acc: 0.9050\n",
      "Epoch 41/100\n",
      "100/100 - 17s - loss: 0.2371 - acc: 0.9000 - val_loss: 0.2547 - val_acc: 0.9010\n",
      "Epoch 42/100\n",
      "100/100 - 17s - loss: 0.2269 - acc: 0.9010 - val_loss: 0.2698 - val_acc: 0.9040\n",
      "Epoch 43/100\n",
      "100/100 - 17s - loss: 0.2164 - acc: 0.9095 - val_loss: 0.3055 - val_acc: 0.8870\n",
      "Epoch 44/100\n",
      "100/100 - 17s - loss: 0.2326 - acc: 0.9010 - val_loss: 0.2480 - val_acc: 0.9100\n",
      "Epoch 45/100\n",
      "100/100 - 16s - loss: 0.2409 - acc: 0.8930 - val_loss: 0.2517 - val_acc: 0.9060\n",
      "Epoch 46/100\n",
      "100/100 - 17s - loss: 0.2312 - acc: 0.9060 - val_loss: 0.2547 - val_acc: 0.9060\n",
      "Epoch 47/100\n",
      "100/100 - 16s - loss: 0.2216 - acc: 0.9050 - val_loss: 0.2643 - val_acc: 0.9080\n",
      "Epoch 48/100\n",
      "100/100 - 17s - loss: 0.2105 - acc: 0.9145 - val_loss: 0.2713 - val_acc: 0.9050\n",
      "Epoch 49/100\n",
      "100/100 - 17s - loss: 0.2278 - acc: 0.9095 - val_loss: 0.2650 - val_acc: 0.9010\n",
      "Epoch 50/100\n",
      "100/100 - 16s - loss: 0.2086 - acc: 0.9135 - val_loss: 0.2726 - val_acc: 0.9070\n",
      "Epoch 51/100\n",
      "100/100 - 17s - loss: 0.2236 - acc: 0.9010 - val_loss: 0.2634 - val_acc: 0.9000\n",
      "Epoch 52/100\n",
      "100/100 - 17s - loss: 0.2230 - acc: 0.9055 - val_loss: 0.2794 - val_acc: 0.8990\n",
      "Epoch 53/100\n",
      "100/100 - 16s - loss: 0.2044 - acc: 0.9095 - val_loss: 0.2648 - val_acc: 0.9050\n",
      "Epoch 54/100\n",
      "100/100 - 16s - loss: 0.2108 - acc: 0.9025 - val_loss: 0.2774 - val_acc: 0.9060\n",
      "Epoch 55/100\n",
      "100/100 - 16s - loss: 0.2070 - acc: 0.9190 - val_loss: 0.2921 - val_acc: 0.8960\n",
      "Epoch 56/100\n",
      "100/100 - 17s - loss: 0.2114 - acc: 0.9090 - val_loss: 0.2777 - val_acc: 0.9050\n",
      "Epoch 57/100\n",
      "100/100 - 17s - loss: 0.2309 - acc: 0.8990 - val_loss: 0.2657 - val_acc: 0.9060\n",
      "Epoch 58/100\n",
      "100/100 - 17s - loss: 0.2173 - acc: 0.9115 - val_loss: 0.3220 - val_acc: 0.8870\n",
      "Epoch 59/100\n",
      "100/100 - 17s - loss: 0.2098 - acc: 0.9090 - val_loss: 0.2770 - val_acc: 0.9000\n",
      "Epoch 60/100\n",
      "100/100 - 17s - loss: 0.1998 - acc: 0.9090 - val_loss: 0.2688 - val_acc: 0.9050\n",
      "Epoch 61/100\n",
      "100/100 - 17s - loss: 0.2081 - acc: 0.9125 - val_loss: 0.2705 - val_acc: 0.9120\n",
      "Epoch 62/100\n",
      "100/100 - 17s - loss: 0.2034 - acc: 0.9170 - val_loss: 0.2801 - val_acc: 0.9030\n",
      "Epoch 63/100\n",
      "100/100 - 17s - loss: 0.2043 - acc: 0.9185 - val_loss: 0.2640 - val_acc: 0.9040\n",
      "Epoch 64/100\n",
      "100/100 - 17s - loss: 0.2031 - acc: 0.9145 - val_loss: 0.2727 - val_acc: 0.9050\n",
      "Epoch 65/100\n",
      "100/100 - 16s - loss: 0.2251 - acc: 0.9030 - val_loss: 0.2661 - val_acc: 0.9060\n",
      "Epoch 66/100\n",
      "100/100 - 17s - loss: 0.2006 - acc: 0.9170 - val_loss: 0.2796 - val_acc: 0.9030\n",
      "Epoch 67/100\n",
      "100/100 - 17s - loss: 0.1956 - acc: 0.9220 - val_loss: 0.2723 - val_acc: 0.9090\n",
      "Epoch 68/100\n",
      "100/100 - 17s - loss: 0.2042 - acc: 0.9190 - val_loss: 0.2724 - val_acc: 0.9070\n",
      "Epoch 69/100\n",
      "100/100 - 17s - loss: 0.2012 - acc: 0.9160 - val_loss: 0.2789 - val_acc: 0.9080\n",
      "Epoch 70/100\n",
      "100/100 - 16s - loss: 0.2041 - acc: 0.9210 - val_loss: 0.2793 - val_acc: 0.9040\n",
      "Epoch 71/100\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "modelFE.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comienza a entrenar el modelo con el método ``fit()``, con la misma configuración de Data Augmentation que utilizamos en la sección anterior.\n",
    "\n",
    "Pero no es necesariamente siempre la mejor opción; la mejor combinación de técnicas depende del tipo y cantidad de datos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "steps_per_epoch = train_generator.n // batch_size\n",
    "validation_steps = validation_generator.n // batch_size\n",
    "\n",
    "historyFE = modelFE.fit(\n",
    "            train_generator,\n",
    "            validation_data = validation_generator,\n",
    "            steps_per_epoch = steps_per_epoch,\n",
    "            epochs = 100,\n",
    "            validation_steps = validation_steps,\n",
    "            verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobar la precisión con los datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mcMMuw_oSRAq"
   },
   "outputs": [],
   "source": [
    "test_lost, test_acc= modelFE.evaluate(test_generator)\n",
    "print (\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0KxNfD7QiJpU"
   },
   "outputs": [],
   "source": [
    "acc      = historyFE.history['acc']\n",
    "val_acc  = historyFE.history[ 'val_acc']\n",
    "loss     = historyFE.history['loss']\n",
    "val_loss = historyFE.history['val_loss']\n",
    "\n",
    "epochs    = range(1,len(acc)+1,1) # obtener número de epochs\n",
    "\n",
    "plt.plot  ( epochs,     acc, 'r--', label='Training acc'  )\n",
    "plt.plot  ( epochs, val_acc,  'b', label='Validation acc')\n",
    "plt.title ('Training and validation accuracy')\n",
    "plt.ylabel('acc')\n",
    "#plt.ylim(0,1)\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.plot  ( epochs,     loss, 'r--' )\n",
    "plt.plot  ( epochs, val_loss ,  'b' )\n",
    "plt.title ('Training and validation loss'   )\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "#plt.ylim(0,1)\n",
    "\n",
    "plt.legend()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Técnica ampliamente utilizada en la reutilización de modelos, complementaria a la Feature Extraction.\n",
    "Consiste en hacer un ajuste más fino y entrenar también algunas de las capas finales de la base convolucional del modelo usado para la extracción de características (que hasta ahora se mantenía congelada), y entrenar conjuntamente tanto la parte agregada del clasificador como estas capas. Esto se llama Fine-Tuning en nuestro entorno porque se trata de un «ajuste fino» de las representaciones más abstractas del modelo que se está reutilizando como base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que el nivel de generalización y, por lo tanto, de reutilización de las representaciones extraídas por capas de convolución específicas depende de la posición de la capa en el modelo.\n",
    "* Las primeras capas aprenden características generales y, después, gradualmente en capas sucesivas se van aprendiendo características más concretas del dominio de problema que estamos tratando.\n",
    "* Las capas del modelo que están más próximas a la capa de entrada de datos extraen mapas de características locales altamente genéricas, mientras que las capas que están más cerca del clasificador final extraen conceptos más abstractos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si nos fijamos en el nombre que se le asigna a las capas en la base convolucional VGG16, vemos que se compone de 5 bloques: block1, block2, block3, block4, block5.\n",
    "\n",
    "Para mostrar la técnica de Fine-Tuning se entrenará el block5 compuesto por tres capas convolucionales y una de pooling (block5_conv1, block5_conv2 y block5_conv3 serán ahora entrenables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1ISi6Wdl4_x"
   },
   "source": [
    "### VGG16 : Modelo con *Fine Tuning*\n",
    "\n",
    "El código en Keras que permite especificar este comportamiento en la fase de entrenamiento que acabamos de describir puede ser el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QZfhhNnptlH0"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "pre_trained_model = VGG16(input_shape = (150, 150, 3), \n",
    "                                include_top = False, \n",
    "                                weights = 'imagenet')\n",
    "\n",
    "pre_trained_model.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "\n",
    "\n",
    "for layer in pre_trained_model.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    if set_trainable: \n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "pre_trained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos igual que antes la red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xN8OOenogf3N"
   },
   "outputs": [],
   "source": [
    "modelFT = Sequential()\n",
    "modelFT.add(pre_trained_model)\n",
    "modelFT.add(Flatten())\n",
    "modelFT.add(Dense(256, activation='relu'))\n",
    "modelFT.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el pre_trained_model tendrá más capas a entrenar, como podemos ver con la salida del método summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFT.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compilación y entrenamiento de la red\n",
    "\n",
    "Se usarán los mismos hiperparámetros (optimizador RMSProp con learning rate de 1e-4), pero es importante dejar claro, para aquellos que quieran profundizar más en el tema, que estos hiperparámetros juegan un papel fundamental.\n",
    "* En general es una buena práctica usar un learning rate muy pequeño para limitar la magnitud de las modificaciones que se realizan en las tres capas del block5 que está ajustando.\n",
    "* Los learning rate que son demasiado grandes pueden dañar los pesos previos que venían de la red preentrenada, ya que mantenían información importante para representar a las características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YuZLpjNFghlD"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "modelFT.compile(loss='binary_crossentropy',\n",
    "              optimizer=RMSprop(lr=1e-4), \n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generación del conjunto de datos aumentados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UnRIx8pDR_wq"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=40,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "\n",
    "validation_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
    "test_datagen  = ImageDataGenerator( rescale = 1.0/255. )\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                                                    batch_size=20,\n",
    "                                                    class_mode='binary',\n",
    "                                                    target_size=(150, 150))     \n",
    "\n",
    "validation_generator =  validation_datagen.flow_from_directory(validation_dir,\n",
    "                                                         batch_size=20,\n",
    "                                                         class_mode  = 'binary',\n",
    "                                                         target_size = (150, 150))\n",
    "\n",
    "\n",
    "test_generator =  test_datagen.flow_from_directory(validation_dir,\n",
    "                                                         batch_size=20,\n",
    "                                                         class_mode  = 'binary',\n",
    "                                                         target_size = (150, 150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fSK81y-tlLa"
   },
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "steps_per_epoch = train_generator.n // batch_size\n",
    "validation_steps = validation_generator.n // batch_size\n",
    "\n",
    "historyFT = modelFT.fit(\n",
    "    train_generator,\n",
    "    validation_data = validation_generator,\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    epochs = 100,\n",
    "    validation_steps = validation_steps,\n",
    "    verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Igual que en los anteriores modelos, se verifica gráficamente cómo evoluciona el entrenamiento en las gráficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xnJPyi2KtlW5"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc      = historyFT.history['acc']\n",
    "val_acc  = historyFT.history['val_acc']\n",
    "loss     = historyFT.history['loss']\n",
    "val_loss = historyFT.history['val_loss']\n",
    "\n",
    "epochs    = range(1,len(acc)+1,1) # obtener número de epochs\n",
    "\n",
    "plt.plot  ( epochs,     acc, 'r--', label='Training acc'  )\n",
    "plt.plot  ( epochs, val_acc,  'b', label='Validation acc')\n",
    "plt.title ('Training and validation accuracy')\n",
    "plt.ylabel('acc')\n",
    "#plt.ylim(0,1)\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.plot  ( epochs,     loss, 'r--' )\n",
    "plt.plot  ( epochs, val_loss ,  'b' )\n",
    "plt.title ('Training and validation loss'   )\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.legend()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con los datos de pruebapara confirmar la mejora observada ya en el historial de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_0-Nfkrtlc0"
   },
   "outputs": [],
   "source": [
    "test_lost, test_acc= modelFT.evaluate(test_generator)\n",
    "print (\"Test Accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jk6mBLCiHr2M"
   },
   "outputs": [],
   "source": [
    "accDA      = historyDA.history['acc']\n",
    "val_accDA  = historyDA.history['val_acc']\n",
    "\n",
    "accFE      = historyFE.history['acc']\n",
    "val_accFE  = historyFE.history['val_acc']\n",
    "\n",
    "accFT      = historyFT.history['acc']\n",
    "val_accFT  = historyFT.history['val_acc']\n",
    "\n",
    "epochs    = range(1,len(accDA)+1,1) \n",
    "\n",
    "plt.figure(figsize=(10,18))\n",
    "\n",
    "plt.plot  ( epochs,     accFT, 'k', label='Fine Tuning - Training acc '  )\n",
    "plt.plot  ( epochs, val_accFT,  'b', label='Fine Tuning - Validation acc ')\n",
    "\n",
    "plt.plot  ( epochs,     accFE, 'r--', label='Feature Extraction - Training acc'  )\n",
    "plt.plot  ( epochs, val_accFE,  'm--', label='Feature Extraction - Validation acc')\n",
    "\n",
    "plt.plot  ( epochs,     accDA, 'g:', label='Data Augmentation - Training acc'  )\n",
    "plt.plot  ( epochs, val_accDA,  'c:', label='Data Augmentation - Validation acc')\n",
    "\n",
    "plt.title ('Training and validation accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.ylim(0.5,1)\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba con fotografías propias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eEr3A3-e3bKZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from google.colab import files\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "uploaded=files.upload()\n",
    "file=list(uploaded.keys())[0]\n",
    "\n",
    "path='/content/' + file\n",
    "img=image.load_img(path, target_size=(150, 150))\n",
    "\n",
    "x=image.img_to_array(img)\n",
    "image=np.expand_dims(x, axis=0)  \n",
    "\n",
    "classes = model.predict(image)\n",
    "print(classes)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "if classes>0: print( fn + \" es un PERRO\")\n",
    "else: print( fn + \" es un GATO\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "11_DataAugmentation_y_TransferLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
