{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--NAVIGATION-->\n",
    "<a href=\"https://colab.research.google.com/github/marcoteran/deeplearningmodule/blob/main/04_recurrentneuralnetworks/04_recurrentneuralnetworks.ipynb\" target=\"_blank\"><img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\" title=\"Abrir y ejecutar en Google Colaboratory\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplos de código\n",
    "# Deep Learning: Redes Neuronales Recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Name:* Marco Teran\n",
    "*E-mail:* marco.tulio.teran@gmail.com,\n",
    "[Website](http://marcoteran.github.io/),\n",
    "[Github](https://github.com/marcoteran),\n",
    "[LinkedIn](https://www.linkedin.com/in/marcoteran/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar librerías importantes\n",
    "\n",
    "Empezamos con las importaciones estándar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#import mlutils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns; sns.set()  # for plot styling\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar TensorFlow 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Qué son las Redes Neuronales Recurrentes?\n",
    "\n",
    "Una **red neuronal recurrente** no tiene una estructura de capas definida, sino que permiten conexiones arbitrarias entre las neuronas, incluso pudiendo crear ciclos, con esto se consigue crear la temporalidad, permitiendo que la red tenga memoria.\n",
    "\n",
    "Las redes neuronales recurrentes son muy potentes para todo lo que tiene que ver con el análisis secuencias, como puede ser el análisis de textos, sonido o video.\n",
    "\n",
    "Existe multitud de tipos de redes neuronales dependiendo del número de capas ocultas y la forma de realizar la retropropagación, a continuación se detallarán las más conocidas y los usos de estas.\n",
    "![Red Neuronal Recurrente](https://www.diegocalvo.es/wp-content/uploads/2017/07/red-neuronal-recurrente-768x251.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo: Programando una RNN generación de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se mostrará cómo se programa una red neuronal recurrente (RNN) basándonos en un caso de estudio que trata de generar texto usando una RNN basada en caracteres. De esta manera, también podemos usar el caso de estudio para mostrar el uso de datos de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo se entrena un modelo de red neuronal para predecir el siguiente carácter a partir de una secuencia de caracteres. Con este modelo intencionadamente simple, para mantener el carácter didáctico del ejemplo, se consiguen generar secuencias de texto más largas llamando al modelo repetidamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-Level Language Models\n",
    "\n",
    "Para intentar buscar un ejemplo lo más simple posible en el que podamos aplicar una red neuronal recurrente, hemos considerado usar el ejemplo de **Character level language model** propuesto por *Andrej Karpathy* en su artículo _«The Unreasonable Effectiveness of Recurrent Neural Networks210»_ (y parcialmente basado en su implementado en el tutorial Generate text with an RNN de la web de TensorFlow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad, se trata de uno de los modelos pioneros en procesado de texto a nivel de carácter, llamado *char-rnn*:\n",
    "* Consiste en darle a la RNN una palabra; entonces se le pide que modele la distribución de probabilidad del siguiente carácter que le correspondería a la secuencia de caracteres anteriores.\n",
    "* Con este modelo, si lo llamamos repetitivamente, podremos generar texto carácter a carácter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ejemplo, supongamos que solo tenemos un vocabulario de cuatro letras posibles [\"a\",\"h\",\"l\",\"o\"], y queremos entrenar a una RNN en la secuencia de entrenamiento \"hola\".\n",
    "\n",
    "Esta secuencia de entrenamiento es, de hecho, una fuente de 3 ejemplos de entrenamiento por separado:\n",
    "* la probabilidad de \"o\" debería ser verosímil dada el contexto de \"h\"\n",
    "* \"l\" debería ser verosímil en el contexto de \"ho\"\n",
    "* \"a\" debería ser también verosímil dado el contexto de \"hol”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este propósito, como dataset usaremos la primera parte del libro **DEEP LEARNING Introducción práctica en Keras213** en texto plano. Se trata de un dataset muy pequeño de solo *30 000* palabras que, además, resulta en parte confuso al pasarlo a texto plano, pues ha resultado una mezcla de texto con código de programación. Pero incluso siendo un dataset extremadamente limitado para poder ser considerado un *corpus real*, nos sirve para generar como salida oraciones en las que —aunque no encontremos ni gramaticalmente ni semánticamente demasiado sentido— se puede apreciar que la estructura del texto de salida se asemeja a una frase real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descarga y preprocesado de los datos\n",
    "\n",
    "El primer paso en este ejemplo será descargar y preparar el conjunto de datos con el que entrenaremos nuestra red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "qEt4CQ0ru7O5",
    "outputId": "a25fcbf9-4b76-4ed7-b654-17963146d458"
   },
   "outputs": [],
   "source": [
    "# Subirlo en línea\n",
    "#from google.colab import files\n",
    "#se debe cargar el fichero “Libro-Deep-Learning-introduccion-practica-con-Keras-1a-parte.txt”\n",
    "#files.upload()\n",
    "\n",
    "# Subirlo manualmente\n",
    "#path_to_fileDL ='data/libro-deep-learning-introduccion-practica-con-keras-1a-parte.txt'\n",
    "\n",
    "# Subirlo automaticamente\n",
    "#path_to_fileDL = tf.keras.utils.get_file('Shakespear.txt', 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt')\n",
    "path_to_fileDL = tf.keras.utils.get_file('libro-deep-learning-introduccion-practica-con-keras-1a-parte', 'https://github.com/marcoteran/deeplearningmodule/raw/main/04_recurrentneuralnetworks/data/libro-deep-learning-introduccion-practica-con-keras-1a-parte.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "pD_55cOxLkAb",
    "outputId": "37cd06ca-04d1-484b-c5a9-a232bffe319c"
   },
   "outputs": [],
   "source": [
    "text = open(path_to_fileDL, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "print('Longitud del texto: {} carácteres'.format(len(text)))\n",
    "\n",
    "vocab = sorted(set(text))\n",
    "\n",
    "print ('El texto está compuesto de estos {} carácteres:'.format(len(vocab)))\n",
    "print (vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como estamos tratando el caso de estudio a nivel de carácter, podríamos considerar que aquí el corpus son los caracteres y, por tanto, sería un corpus muy pequeño. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que las redes neuronales solo procesan valores numéricos, no letras; por tanto, tenemos que traducir los caracteres a representación numérica. Para ello crearemos dos «tablas de traducción»: una de caracteres a números y otra de números a caracteres: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos un token con la representación de entero (integer) para cada carácter, como podemos ver ejecutando el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IalZLbvOzf-F",
    "outputId": "fd81ff07-a95d-46f7-942d-6cbffc0c3989"
   },
   "outputs": [],
   "source": [
    "for char,_ in zip(char2idx, range(len(vocab))):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y con esta función, inversa a la anterior, podemos pasar el texto (todo el libro) a enteros: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ML-DuUyi_aHy"
   },
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprobarlo podemos mostrar los 50 primeros caracteres del texto contenido en el tensor text_as_int: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "l1VKcQHcymwb",
    "outputId": "0ef61420-a0eb-4164-f732-37239c69cdf0"
   },
   "outputs": [],
   "source": [
    "print ('texto: {}'.format(repr(text[:50])))\n",
    "print ('{}'.format(repr(text_as_int[:50])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### Preparación de los datos para ser usados por la RNN\n",
    "\n",
    "Para entrenar el modelo, prepararemos unas secuencias de caracteres como entradas y salida de un tamaño determinado.\n",
    "* En nuestro ejemplo, hemos definido el tamaño de **100 caracteres** con la variable ``seq_length``\n",
    "* Empezamos dividiendo el texto que tenemos en secuencias de ``seq_length+1`` de caracteres\n",
    "* Luego construiremos los datos de entrenamiento compuestos por las entradas de ``seq_length`` caracteres y las salidas correspondientes que contienen la misma longitud de texto (excepto que se desplaza un carácter a la derecha).\n",
    "* Ejemplo: “Hola”. Suponiendo un ``seq_length=3``, la secuencia de entrada será ``\"Hol\"``, y la de salida será ``\"ola\"``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos la función ``tf.data.Dataset.from_tensor_slices``, que crea un conjunto de datos con el contenido del ``tensortext_as_int`` que contiene el texto, al que podremos aplicar el método ``batch()`` para dividir este conjunto de datos en secuencias de seq_length+1 de índice de caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UHJDA39zf-O"
   },
   "outputs": [],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "seq_length = 100\n",
    " \n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comprobar que sequences contiene el texto dividido en paquetes de 101 caracteres como esperamos (por ejemplo, mostremos las 10 primeras secuencias):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "colab_type": "code",
    "id": "l4hkDU3i7ozi",
    "outputId": "4b0f9234-3e2c-4a97-c253-b271d82b9520"
   },
   "outputs": [],
   "source": [
    "for item in sequences.take(10):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta secuencia se obtiene el conjunto de datos de entrenamiento que contiene tanto los datos de entrada (desde la posición 0 a la 99) como los datos de salida (desde la posición 1 a la 100).\n",
    "\n",
    "Para ello, se crea una función que realiza esta tarea y se aplica a todas las secuencias usando el método ``map()`` de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, dataset contiene un conjunto de parejas de secuencias de texto (con la representación numérica de los caracteres) donde el primer componente de la pareja contiene un paquete con una secuencia de 100 caracteres del texto original, y el segundo contiene su correspondiente salida (etiqueta), también de 100 caracteres. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comprobarlo visualizándolo por pantalla (por ejemplo mostrando la primera pareja): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "GNbw-iR0ymwj",
    "outputId": "fda91b1a-1845-46dd-915e-490953b99144"
   },
   "outputs": [],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto del código disponemos de los datos de entrenamiento en el tensor ``dataset`` en forma de parejas de secuencias de 100 integers de 64 bits que representan un carácter del vocabulario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0eBu9WZG84i0",
    "outputId": "fe45c87d-9085-463f-bfd1-a4f23d2c2a8f"
   },
   "outputs": [],
   "source": [
    "print (dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Los datos ya están preprocesados en el formato que se requiere para ser usados en el entreno de la red neuronal\n",
    "* **Recordar:** en redes neuronales los datos se agrupan en *batches* antes de pasarlos al modelo.\n",
    "    * En nuestro caso hemos decidido un *tamaño de batch* de 64, que nos facilita la explicación pero. Este es un hiperparámetro importante, y para ajustarlo correctamente hay que tener en cuenta diferentes factores, como el tamaño de la memoria disponible\n",
    "    * Para crear los batches de parejas de secuencias hemos considerado usar ``tf.data`` que, además, nos permite *barajar* las secuencias previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "p2pGotuNzf-S",
    "outputId": "dee41632-e0b6-4102-89c1-c37064974de0"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print (dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el tensor ``dataset`` disponemos de los datos de entrenamiento ya listos para ser usados para ajustar los parámetros del modelo con el proceso de entrenamiento:\n",
    "* *batches* compuestos de 64 parejas de secuencias de 100 integers de 64 bits que representan el carácter correspondiente en el vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "### Construcción del modelo RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir el modelo usaremos la API secuencial de Keras ``tf.keras.Sequential``.\n",
    "Usaremos una versión mínima de RNN para facilitar la explicación, que contenga solo una capa LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En concreto, definimos una red de solo 3 capas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pKJl2dqwHRl"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, \n",
    "                        output_dim=embedding_dim,\n",
    "                        batch_input_shape=[batch_size, None]))\n",
    "    model.add(LSTM(rnn_units,\n",
    "                   return_sequences=True,\n",
    "                   stateful=True,\n",
    "                   recurrent_initializer='glorot_uniform'))\n",
    "    model.add(Dense(vocab_size))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. La primera capa es de tipo ``word embedding``, como las que antes hemos presentado muy brevemente, que mapea cada carácter de entrada en un vector embedding. Esta ``capa tf.keras.layers.Embedding`` permite especificar varios argumentos que se pueden consultar en todo detalle en el manual de TensorFlow.\n",
    "    * En nuestro caso, el primero que especificamos es el tamaño del vocabulario, indicado con el argumento ``vocab_size``, que indica cuántos vectores embedding tendrá la capa.\n",
    "    * A continuación, indicamos las dimensiones de estos vectores embedding mediante el argumento ``embedding_dim``, que en nuestro caso hemos decidido que sea 256.\n",
    "    * Finalmente, se indica el tamaño del batch que usaremos para entrenar, en nuestro caso 64.\n",
    "\n",
    "2. La segunda capa es de tipo LSTM. Esta capa ``tf.keras.layers.LSTM`` tiene varios argumentos posibles que se pueden consultar en el manual de TensorFlow; aquí solo usaremos algunos y dejaremos los valores por defecto del resto.\n",
    "    * Quizás el más importante sea el número de neuronas recurrentes, que se indica con el argumento ``units`` y que en nuestro caso hemos decidido que sea 1024 neuronas.\n",
    "    * Con ``return_sequence`` se indica que queremos predecir el carácter siguiente a todos los caracteres de entrada, no solo el siguiente al último carácter.\n",
    "    * El argumento ``stateful`` indica el uso de las capacidades de memoria de la red entre batches. Si este argumento está instanciado a **``False``** se indica que a cada nuevo batch se inicializan las memory cells, mientras que si está a **``True``** se está indicando que para cada batch se mantendrán las actualizaciones hechas durante la ejecución del batch anterior.\n",
    "    * El último argumento que usamos es ``recurrent_kernel``, donde indicamos cómo se deben inicializar los pesos de las matrices internas de la red. En este caso usamos la distribución uniforme ``glorot_uniform``, habitual en estos casos.\n",
    "\n",
    "3. Finalmente la última capa es de tipo ``Dense``. Aquí es importante el argumento ``units`` que nos dice cuántas neuronas tendrá la capa y que nos marcará la dimensión de la salida. En nuestro caso será igual al tamaño de nuestro vocabulario ``(vocab_size)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwsrpOik5zhv"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es interesante usar el método summary() para visualizar la estructura del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "PpHGXDMcZ0Zt",
    "outputId": "086470f9-57f7-4d3d-d446-55f945fed488"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Podemos comprobar que la capa LSTM consta de muchos parámetros (más de 5 millones) como era de esperar.\n",
    "* Para cada carácter de entrada (transformado a su equivalente numérico), el modelo busca su vector de embedding correspondiente, y luego ejecuta la capa LSTM con este vector embedding como entrada.\n",
    "* A la salida de la LSTM aplica la capa Dense para decidir cuál es el siguiente carácter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspeccionemos las dimensiones de los tensores para poder comprender más a fondo el modelo. Fijémonos en el primer batch del conjunto de datos de entrenamiento y observemos su forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "C-_70kKAPrPU",
    "outputId": "b71d7b4f-c635-4774-f058-d67067a72eae"
   },
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  print(\"Input:\", input_example_batch.shape, \"# (batch_size, sequence_length)\")\n",
    "  print(\"Target:\", target_example_batch.shape, \"# (batch_size, sequence_length)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que en esta red la secuencia de entrada son batches de 100 caracteres, pero el modelo una vez entrenado puede ser ejecutado con cualquier tamaño de cadena de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como salida, el modelo nos devuelve un tensor con una dimensión adicional con la verosimilitud para cada carácter del vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "udE_6eTldMhB",
    "outputId": "56ea08dd-4b13-45dd-d665-04c6bcc7c58a"
   },
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):  \n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(\"Prediction: \", example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso es elegir uno de los caracteres. No se eligirá el carácter más *«probable»* (mediante ``argmax``), puesto que el modelo puede entrar en un bucle.\n",
    "\n",
    "Lo que se hará es obtener una muestra de la distribución de salida. Prueba para el primer ejemplo en el batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices_characters = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "YqFMUQc_UFgM",
    "outputId": "62cd2cbb-80bc-4f6d-9161-6471e8eb4d4d"
   },
   "outputs": [],
   "source": [
    "sampled_indices_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con ``tf.random.categorical`` se obtiene una muestra de una distribución categórica y con ``squeeze`` se eliminan la dimensiones del tensor de tamaño 1.\n",
    "Así, en cada instante de tiempo se obtiene una predicción del índice del siguiente carácter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trpqTWyvk0nr"
   },
   "source": [
    "## Entrenar del modelo RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, el problema puede tratarse como un problema de clasificación estándar para el que debemos definir la función de pérdida y el optimizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la función de pérdida usaremos la función estándar ``tf.keras.losses.sparse_categorical_crossentropy``, puesto que estamos considerando datos categóricos. Dado que el retorno hemos visto que se trata de unos valores de verosimilitud (no de probabilidades —como si hubiéramos ya aplicado softmax—) se instanciará el argumento ``from_logits`` a **``True``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HrXTACTdzY-"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el optimizador, usaremos ``tf.keras.optimizers.Adam`` con los argumentos por defecto del optimizador Adam. Con la función de lossdefinida, y usando el optimizador Adam con sus argumentos por defecto, ya podemos llamar al método ``compile()`` de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieSJdchZggUj"
   },
   "source": [
    "#### Configurar el *checkpoints*\n",
    "\n",
    "En este ejemplo aprovecharemos para usar los *Checkpoints*:\n",
    "* Una técnica de tolerancia de fallos para procesos cuyo tiempo de ejecución es muy largo.\n",
    "* La idea es guardar una instantánea del estado del sistema *periódicamente* para recuperar desde ese punto la ejecución en caso de fallo del sistema.\n",
    "\n",
    "Cuando entrenamos modelos Deep Learning, el *Checkpoint* lo forman básicamente los pesos del modelo:\n",
    "* Estos Checkpoint se pueden usar también para hacer predicciones\n",
    "\n",
    "La librería de Keras proporciona *Checkpoints* a través de la API ``Callbacks``. Concretamente usaremos ``tf.keras.callbacks.ModelCheckpoint`` para especificar cómo salvar los *Checkpoints* a cada *epoch* durante el entrenamiento, a través de un argumento en el método ``fit()`` del modelo.\n",
    "* En el código debemos especificar el directorio en el que se guardarán los *Checkpoints* que salvaremos y el nombre del fichero (al que añadiremos el número de *epoch* para nuestra comodidad): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    " # directorio\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# nombre fichero\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora ya está todo preparado para empezar a entrenar la red con el método ``fit()``: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UK-hmKjYVoll",
    "outputId": "1ecb5c0b-068a-41e5-9e13-e47a432035cf"
   },
   "outputs": [],
   "source": [
    "EPOCHS=50\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede inspeccionar el contenido del directorio training_checkpoints para comprobar que se han generado estos ficheros mediante el comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls training_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## Generación de texto usando el modelo RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya tenemos entrenado el modelo, pasemos a usarlo para generar texto.\n",
    "Para mantener este paso de predicción simple, vamos a usar un tamaño de *batch* de 1.\n",
    "Debido a la forma en que se pasa el estado de la RNN de un instante de tiempo al siguiente, el modelo solo acepta un tamaño de batch fijo una vez construido.\n",
    "Por ello, para poder ejecutar el modelo con un tamaño de *batch* diferente, necesitamos reconstruir manualmente el modelo con el método ``build()`` y restaurar sus pesos desde el *Checkpoint* (cogemos el ultimo con ``tf.train.latest_checkpoint()``)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zk2WJ2-XjkGz",
    "outputId": "ef8b7f87-a25c-4b48-e662-cc13a2876a30"
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos el modelo entrenado y preparado para usar, generaremos texto a partir de una palabra de partida con el siguiente código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código empieza con inicializaciones como:\n",
    "* Definir el número de caracteres a predecir con la variable ``num_generate``\n",
    "* Convertir la palabra inicial (``start_string``) a su correspondiente representación numérica y preparar lo tensores necesarios\n",
    "```Python\n",
    "num_generate = 500\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "```\n",
    "Usando la misma idea del código original **char-rnn** de *Andrey Karpathy*, se usa una variable ``temperature`` para decidir cuán conservador en sus predicciones queremos que sea nuestro modelo. En nuestro ejemplo la hemos inicializado a 0.5\n",
    "* Con *«temperaturas altas»* (hasta 1) se permitirá más creatividad al modelo para generar texto, pero a costa de más errores (por ejemplo, errores ortográficos, etc.).\n",
    "* Con *«temperaturas bajas»* habrá menos errores pero el modelo mostrará poca creatividad.\n",
    "Es posible revisar diferentes valores y ver su efecto\n",
    "```Python\n",
    "temperature = 0.5``\n",
    "```\n",
    "A partir de este momento empieza el bucle para generar los caracteres que le hemos indicado (que use el carácter de entrada la primera vez) y luego sus propias predicciones como entrada a cada nueva iteración al modelo RNN:\n",
    "```Python\n",
    " model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "```\n",
    "Recordemos que estamos en un batch de tamaño 1 pero el modelo retorna el tensor del batch con las dimensiones con que lo habíamos entrenado y, por tanto, debemos reducir la dimensión batch:\n",
    "```Python\n",
    "predictions = tf.squeeze(predictions, 0)\n",
    "```\n",
    "Luego, se usa una distribución categórica para calcular el índice del carácter predicho: \n",
    "```Python\n",
    "predictions = predictions / temperature\n",
    "predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "```\n",
    "Este carácter acabado de predecir se usa como nuestra próxima entrada al modelo, retroalimentando el modelo para que ahora tenga más contexto (en lugar de una sola letra).\n",
    "\n",
    "Después de predecir la siguiente letra, se retroalimenta nuevamente, y así sucesivamente, de manera que aprende a medida que se obtiene más contexto de los caracteres predichos previamente:\n",
    "```Python\n",
    "input_eval = tf.expand_dims([predicted_id], 0)\n",
    "text_generated.append(idx2char[predicted_id])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \n",
    "    num_generate = 500\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    text_generated = []\n",
    "\n",
    "\n",
    "    temperature = 0.5\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "      \n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que se ha descrito cómo se ha programado la función ``generate_txt`` probemos cómo se comporta el modelo.\n",
    "Empecemos con una palabra que no conoce el corpus, por ejemplo **«Domingo»**, que nada tiene que ver con *Deep Learning*, aunque es a quien hemos dedicado este libro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "ktovv0RFhrkn",
    "outputId": "2b110c21-3286-4f43-98fa-7bcda1a9f209"
   },
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string=u\"Domingo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos ahora con una palabra como «Activación» o «Redes», a ver qué pasa: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string=u\"Activacion\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string=u\"Redes\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "04_recurrentneuralnetworks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
